{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"计算 softmax\"\"\"\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "def masked_attention(query, key, value, mask):\n",
    "    \"\"\"\n",
    "    计算带有 mask 的注意力机制\n",
    "    :param query: 查询矩阵 (batch_size, seq_len, d_k)\n",
    "    :param key: 键矩阵 (batch_size, seq_len, d_k)\n",
    "    :param value: 值矩阵 (batch_size, seq_len, d_v)\n",
    "    :param mask: mask 矩阵 (batch_size, 1, seq_len) 或 (batch_size, seq_len, seq_len)\n",
    "    :return: 注意力输出\n",
    "    \"\"\"\n",
    "    # 计算注意力分数\n",
    "    scores = np.matmul(query, key.transpose(0, 2, 1)) / np.sqrt(query.shape[-1])\n",
    "    \n",
    "    # 应用 mask\n",
    "    scores += (mask * -1e9)  # 将 mask 为 0 的位置设置为一个很小的值\n",
    "\n",
    "    # 计算注意力权重\n",
    "    attention_weights = softmax(scores)\n",
    "\n",
    "    # 计算注意力输出\n",
    "    output = np.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "# 示例数据\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_k = 4\n",
    "d_v = 4\n",
    "\n",
    "# 随机生成查询、键、值和 mask\n",
    "query = np.random.rand(batch_size, seq_len, d_k)\n",
    "key = np.random.rand(batch_size, seq_len, d_k)\n",
    "value = np.random.rand(batch_size, seq_len, d_v)\n",
    "\n",
    "# 创建一个 mask，假设我们只想关注前 3 个位置\n",
    "mask = np.zeros((batch_size, 1, seq_len))\n",
    "mask[:, :, 3:] = 1  # 将后面的位置设置为 1\n",
    "\n",
    "# 计算带有 mask 的注意力\n",
    "output, attention_weights = masked_attention(query, key, value, mask)\n",
    "\n",
    "print(\"注意力输出:\\n\", output)\n",
    "print(\"注意力权重:\\n\", attention_weights)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
